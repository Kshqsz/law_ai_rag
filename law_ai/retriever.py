# coding: utf-8
"""
æ³•å¾‹æ–‡æ¡£æ£€ç´¢æ¨¡å—ï¼ˆåŒ…æ‹¬å‘é‡æ£€ç´¢å’Œç½‘é¡µæœç´¢ï¼‰

åŠŸèƒ½è¯´æ˜Žï¼š
- ProxyDuckDuckGoSearch: æ”¯æŒä»£ç†çš„ DuckDuckGo æœç´¢å·¥å…·
  é€šè¿‡ html å’Œ lite ä¸¤ä¸ªåŽç«¯è¿›è¡Œç½‘é¡µæœç´¢
  æ”¯æŒè‡ªå®šä¹‰ä»£ç†å’Œè¶…æ—¶æ—¶é—´
  è‡ªåŠ¨æ·»åŠ å»¶è¿Ÿä»¥é¿å…è§¦å‘ API é™é€Ÿ
  
- LawWebRetiever: ç½‘é¡µæ£€ç´¢å™¨ï¼ˆç»§æ‰¿ BaseRetrieverï¼‰
  ä½¿ç”¨ DuckDuckGo æœç´¢ç½‘é¡µç»“æžœ
  è‡ªåŠ¨å¯¹æœç´¢ç»“æžœè¿›è¡Œæ–‡æœ¬åˆ†å‰²

ä½¿ç”¨ç¤ºä¾‹ï¼š
    from law_ai.retriever import ProxyDuckDuckGoSearch, LawWebRetiever
    from law_ai.utils import get_vectorstore
    
    # ç¤ºä¾‹ 1: ç›´æŽ¥ä½¿ç”¨ ProxyDuckDuckGoSearch
    search = ProxyDuckDuckGoSearch(proxy="http://127.0.0.1:7890")
    results = search.results("ä¸­å›½æ°‘æ³•å…¸ åˆåŒ", max_results=3)
    
    print(f"æ‰¾åˆ° {len(results)} æ¡ç»“æžœï¼š")
    for i, result in enumerate(results, 1):
        print(f"{i}. æ ‡é¢˜: {result['title']}")
        print(f"   é“¾æŽ¥: {result['link']}")
        print(f"   æ‘˜è¦: {result['snippet'][:100]}...")
    
    # è¾“å‡ºç¤ºä¾‹:
    # æ‰¾åˆ° 3 æ¡ç»“æžœï¼š
    # 1. æ ‡é¢˜: ã€Šä¸­åŽäººæ°‘å…±å’Œå›½æ°‘æ³•å…¸ã€‹ç¬¬ä¸‰ç¼– åˆåŒ
    #    é“¾æŽ¥: https://www.spp.gov.cn/...
    #    æ‘˜è¦: ç¬¬å››ç™¾å…­åä¸‰æ¡ æœ¬ç¼–è°ƒæ•´å› åˆåŒäº§ç”Ÿçš„æ°‘äº‹å…³ç³»...
    
    # ç¤ºä¾‹ 2: ä½¿ç”¨ LawWebRetiever è¿›è¡Œæ£€ç´¢
    from langchain.callbacks.manager import CallbackManagerForRetrieverRun
    
    vectorstore = get_vectorstore("web")
    retriever = LawWebRetiever(
        vectorstore=vectorstore,
        search=search,
        num_search_results=2
    )
    
    # èŽ·å–ç›¸å…³æ–‡æ¡£
    # docs = retriever.get_relevant_documents("åˆåŒçš„è¿çº¦è´£ä»»å¦‚ä½•å¤„ç†ï¼Ÿ")
"""
import os
from typing import List, Optional

from langchain.schema.vectorstore import VectorStore
from langchain.schema import BaseRetriever, Document
from langchain.callbacks.manager import CallbackManagerForRetrieverRun
from langchain.pydantic_v1 import Field, BaseModel
from langchain.output_parsers import PydanticOutputParser
from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter
from langchain.chains import LLMChain
from langchain.retrievers.multi_query import MultiQueryRetriever
from duckduckgo_search import DDGS
from duckduckgo_search.exceptions import DuckDuckGoSearchException

from .prompt import MULTI_QUERY_PROMPT_TEMPLATE
from .utils import get_model
from .logger import retriever_logger


class ProxyDuckDuckGoSearch:
    """æ”¯æŒä»£ç†çš„ DuckDuckGo æœç´¢"""
    
    def __init__(self, proxy: Optional[str] = None, timeout: int = 15):
        self.proxy = proxy or os.getenv("HTTPS_PROXY") or os.getenv("HTTP_PROXY")
        self.timeout = timeout
    
    def results(self, query: str, max_results: int = 2) -> List[dict]:
        import time
        import random
        
        # å°è¯• html å’Œ lite åŽç«¯ï¼ˆapi å®¹æ˜“é™é€Ÿï¼‰
        backends = ["html", "lite"]
        
        for backend in backends:
            try:
                delay = random.uniform(2, 4)
                retriever_logger.debug(f"  ðŸ¦† DuckDuckGo ({backend}) ç­‰å¾… {delay:.1f}s...")
                time.sleep(delay)
                
                with DDGS(proxies=self.proxy, timeout=self.timeout) as ddgs:
                    results = list(ddgs.text(query, max_results=max_results, backend=backend))
                    formatted = []
                    for r in results:
                        formatted.append({
                            "title": r.get("title", ""),
                            "link": r.get("href", ""),
                            "snippet": r.get("body", "")
                        })
                    if formatted:
                        retriever_logger.info(f"âœ“ DuckDuckGo ({backend}) æˆåŠŸæ‰¾åˆ° {len(formatted)} æ¡ç»“æžœ")
                        return formatted
            except Exception as e:
                retriever_logger.debug(f"  âœ— {backend} å¤±è´¥: {str(e)[:50]}")
                continue
        
        retriever_logger.warning(f"âš  ç½‘é¡µæœç´¢å¤±è´¥: æ‰€æœ‰åŽç«¯å‡æ— å¯ç”¨ç»“æžœ")
        return []


class LawWebRetiever(BaseRetriever):
    # Inputs
    vectorstore: VectorStore = Field(
        ..., description="Vector store for storing web pages"
    )

    search: ProxyDuckDuckGoSearch = Field(..., description="DuckDuckGo Search with Proxy")
    num_search_results: int = Field(1, description="Number of pages per search")

    text_splitter: TextSplitter = Field(
        RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50),
        description="Text splitter for splitting web pages into chunks",
    )
    
    class Config:
        arbitrary_types_allowed = True

    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun,
    ) -> List[Document]:
        retriever_logger.info(f"ðŸ” å¼€å§‹ç½‘é¡µæœç´¢: '{query}'")

        results = self.search.results(query, self.num_search_results)
        if results:
            retriever_logger.info(f"âœ“ ç½‘é¡µæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æžœ")
            for i, res in enumerate(results, 1):
                retriever_logger.info(f"  ðŸ“„ ç½‘é¡µ{i}: {res.get('title', 'N/A')[:50]}...")

        docs = []
        for res in results:
            docs.append(Document(
                page_content=res["snippet"],
                metadata={"link": res["link"], "title": res["title"]}
            ))

        docs = self.text_splitter.split_documents(docs)

        return docs


# Output parser will split the LLM result into a list of queries
class LineList(BaseModel):
    # "lines" is the key (attribute name) of the parsed output
    lines: List[str] = Field(description="Lines of text")


class LineListOutputParser(PydanticOutputParser):
    def __init__(self) -> None:
        super().__init__(pydantic_object=LineList)

    def parse(self, text: str) -> LineList:
        lines = text.strip().split("\n")
        return LineList(lines=lines)


def get_multi_query_law_retiever(retriever: BaseRetriever, model: BaseModel) -> BaseRetriever:
    output_parser = LineListOutputParser()

    llm_chain = LLMChain(llm=model, prompt=MULTI_QUERY_PROMPT_TEMPLATE, output_parser=output_parser)

    retriever = MultiQueryRetriever(
        retriever=retriever, llm_chain=llm_chain, parser_key="lines"
    )

    return retriever
